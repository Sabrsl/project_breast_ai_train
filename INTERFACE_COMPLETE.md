# ğŸ¯ Interface ComplÃ¨te : Tous les ParamÃ¨tres ContrÃ´lables

## ğŸ“‹ Vue d'ensemble

L'interface web `frontend/app.html` permet de contrÃ´ler **TOUS** les aspects de l'entraÃ®nement depuis le navigateur. Aucune modification du code n'est nÃ©cessaire.

---

## ğŸ”§ ParamÃ¨tres ContrÃ´lables

### 1. **PrÃ©sets Ultra-Performance** âš¡
| Preset | Description | Temps estimÃ© | AUC attendu |
|--------|-------------|--------------|-------------|
| **ULTRA-S** | EfficientNetV2-S, rapide | ~30 jours CPU | 0.970-0.973 |
| **ULTRA-M** | EfficientNetV2-M, optimal | ~50 jours CPU | 0.975-0.978 |
| **Custom** | Configuration manuelle | Variable | Variable |

**Auto-remplissage** : Changez le preset et tous les champs se mettent Ã  jour automatiquement !

---

### 2. **ModÃ¨le** ğŸ—ï¸
- `EfficientNetV2-S` : Petit, rapide, AUC ~0.97
- `EfficientNetV2-M` : Moyen, Ã©quilibrÃ©, AUC ~0.975
- `EfficientNetV2-L` : Grand, lent, AUC ~0.98+
- `EfficientNet-B4` : Legacy, performances moyennes

---

### 3. **HyperparamÃ¨tres de Base** ğŸ“Š

| ParamÃ¨tre | Valeur par dÃ©faut | Plage | Description |
|-----------|-------------------|-------|-------------|
| **Epochs** | 80 | 1-200 | Nombre de passages sur le dataset |
| **Batch Size** | 4 | 1-16 | Images par batch (limitÃ© par RAM) |
| **Learning Rate** | 0.0003 | 0.00001-0.01 | Vitesse d'apprentissage |
| **Weight Decay** | 0.0001 | 0-0.01 | RÃ©gularisation L2 |
| **Label Smoothing** | 0.12 | 0-0.3 | Lissage des labels (0.12 = clinique) |
| **Dropout Rate** | 0.45 | 0.1-0.8 | Taux de dropout dans la tÃªte |

---

### 4. **Optimisation** ğŸš€

#### **Optimizer**
- `AdamW` â­ (recommandÃ©) : Meilleure gÃ©nÃ©ralisation
- `Adam` : Plus rapide mais moins stable
- `SGD` : Robuste mais lent

#### **Scheduler**
- `Cosine Annealing` : DÃ©croissance progressive du LR

---

### 5. **Features AvancÃ©es** ğŸ”¥

| Feature | Options | Description | Impact |
|---------|---------|-------------|--------|
| **CBAM** | âœ… ActivÃ© / âŒ DÃ©sactivÃ© | Attention spatiale et canal | +1-2% AUC |
| **Focal Loss** | âœ… ActivÃ© / âŒ CrossEntropy | PrioritÃ© aux cas difficiles (malignant) | +1-2% sensibilitÃ© |
| **TTA** | âœ… ActivÃ© / âŒ DÃ©sactivÃ© | 6 augmentations Ã  l'infÃ©rence | +1-1.5% AUC |
| **EMA** | âœ… ActivÃ© / âŒ DÃ©sactivÃ© | Lissage exponentiel des poids | +0.5-1% stabilitÃ© |
| **Gradient Acc** | 1-16 steps | Batch effectif = Batch Ã— Steps | Simule gros batch |

---

### 6. **Progressive Unfreezing** ğŸ§Šâ¡ï¸ğŸ”¥

**Automatique selon l'epoch** :
- **Phase 1** (Epochs 1-8) : Backbone gelÃ©, seule la tÃªte est entraÃ®nÃ©e
- **Phase 2** (Epochs 9-20) : DÃ©gel progressif des derniers blocs
- **Phase 3** (Epochs 21-40) : DÃ©gel des blocs moyens
- **Phase 4** (Epochs 41+) : Tout dÃ©gelÃ©, fine-tuning complet

---

## ğŸ¯ Configurations RecommandÃ©es

### âš¡ ULTRA-S : Rapide (30 jours)
```
ModÃ¨le           : EfficientNetV2-S
Epochs           : 80
Batch Size       : 4
Learning Rate    : 0.0003
Weight Decay     : 0.0001
Dropout          : 0.45
Label Smoothing  : 0.12
Optimizer        : AdamW
CBAM             : âœ…
Focal Loss       : âœ…
TTA              : âœ…
EMA              : âœ…
Gradient Acc     : 4

Performance attendue : AUC 0.970-0.973
```

### ğŸ† ULTRA-M : Optimal (50 jours)
```
ModÃ¨le           : EfficientNetV2-M
Epochs           : 80
Batch Size       : 4
Learning Rate    : 0.0003
Weight Decay     : 0.0001
Dropout          : 0.45
Label Smoothing  : 0.12
Optimizer        : AdamW
CBAM             : âœ…
Focal Loss       : âœ…
TTA              : âœ…
EMA              : âœ…
Gradient Acc     : 4

Performance attendue : AUC 0.975-0.978
```

---

## ğŸ”„ Workflow

### 1. **Choisir un Preset**
- SÃ©lectionnez `âš¡ ULTRA-S` ou `ğŸ† ULTRA-M` dans le dropdown
- **Tous les champs se remplissent automatiquement** !

### 2. **Ajuster (Optionnel)**
- Mode `Custom` : Modifiez manuellement chaque paramÃ¨tre
- Affinez selon votre matÃ©riel (RAM, CPU)

### 3. **DÃ©marrer l'EntraÃ®nement**
- Cliquez sur `ğŸš€ DÃ©marrer EntraÃ®nement`
- Les logs apparaissent **en temps rÃ©el** (chaque batch)

### 4. **Suivre les MÃ©triques**
- **Graphiques en temps rÃ©el** : Loss, Accuracy, F1-Score
- **Logs Console** : Batch progress, validation metrics
- **Checkpoints automatiques** : Meilleur modÃ¨le sauvegardÃ©

---

## ğŸ› ï¸ Troubleshooting

### â“ Quel preset choisir ?

| Cas d'usage | Preset recommandÃ© |
|-------------|-------------------|
| **Prototypage rapide** | ULTRA-S (30j) |
| **Usage clinique** | ULTRA-M (50j) |
| **Recherche maximale** | Custom avec EfficientNetV2-L |

### â“ Batch Size trop grand ?

```
Erreur : "CUDA out of memory" ou "RuntimeError: DataLoader"

Solution :
1. RÃ©duire Batch Size : 4 â†’ 2 â†’ 1
2. Augmenter Gradient Acc : 4 â†’ 8 â†’ 16
   (Batch effectif reste le mÃªme !)
```

### â“ Training trop lent ?

```
Solutions :
1. VÃ©rifier GPU : torch.cuda.is_available() doit Ãªtre True
2. RÃ©duire Epochs : 80 â†’ 50 â†’ 30
3. Changer de modÃ¨le : EfficientNetV2-M â†’ S
4. DÃ©sactiver TTA (seulement en validation)
```

---

## ğŸ“Š MÃ©triques AffichÃ©es

### Pendant l'EntraÃ®nement
- **Batch Progress** : Loss, temps/batch
- **Epoch Summary** : Acc train, Loss train
- **Validation** : Acc, F1-macro, F1-weighted, Precision, Recall

### En Fin d'EntraÃ®nement
- **Best Model** : Epoch du meilleur modÃ¨le
- **Final Metrics** : AUC, SensibilitÃ©, SpÃ©cificitÃ©, PPV, NPV
- **Checkpoint** : Sauvegarde automatique dans `checkpoints/`

---

## ğŸš€ Prochaines Ã‰tapes

1. **Ouvrir l'interface** : `python server_simple.py`
2. **Naviguer** : http://localhost:8765
3. **Choisir ULTRA-M** (ou ULTRA-S pour tester)
4. **DÃ©marrer** et attendre 30-50 jours ğŸ•
5. **Profiter d'un modÃ¨le clinique de classe mondiale** ! ğŸ†

---

**Note** : Tous les paramÃ¨tres sont synchronisÃ©s entre l'interface â†’ serveur â†’ training. Aucune modification de code nÃ©cessaire !
